{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/cookbook/blob/main/information-retrieval/sentence-transformers/v3-fine-tuning.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/aurelio-labs/cookbook/blob/main/information-retrieval/sentence-transformers/v3-fine-tuning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Embedding Models with Sentence Transformers 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this blog post, we will walk through the process of fine-tuning embedding models using Sentence Transformers 3 to enhance Retrieval-Augmented Generation (RAG) performance.\n",
    "\n",
    "## Install the Necessary Libraries\n",
    "Install the following libraries:\n",
    "- Pytorch\n",
    "- Sentence Transformers (HF)\n",
    "- Transformers (HF)\n",
    "- Datasets (HF)\n",
    "\n",
    "We are currently using Python 3.11.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerate launch –multi-gpu –num_processes=2 v3_ft_embedding.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --qU \\\n",
    "    \"torch==2.2.2\" \\\n",
    "    \"tensorboard==2.17.0\" \\\n",
    "    \"sentence-transformers==3.0.1\" \\\n",
    "    \"datasets==2.19.1\"  \\\n",
    "    \"transformers==4.41.2\" \\\n",
    "    \"accelerate==0.31.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing the necessary libraries, you should register on [Hugging Face](https://huggingface.co/join) as we are going to use Hugging Face Hub to push our models and training logs.\n",
    "\n",
    "Get your access token [here](https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log into your HF account and store your token (access key) on the disk\n",
    "from huggingface_hub import login\n",
    "\n",
    "# login(token=\"ADD YOUR TOKEN HERE\", add_to_git_credential=True)\n",
    "login(token=\"ADD YOUR TOKEN HERE\", add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"]= \"...\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"Fine-tune model with Sentence Transformer\"\n",
    "os.environ[\"WANDB_NAME\"] = \"ft-with-st-v3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载预训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "\n",
    "The Hugging Face Hub has a lot of datasets that can be used to fine-tune embeddings models.You can take a look [here](https://sbert.net/docs/sentence_transformer/dataset_overview.html) at what sort of dataset structure should your dataset follow in order to be able to use it for fine-tunning embeddings.\n",
    "\n",
    "We are going to use [enelpol/rag-mini-bioasq](https://huggingface.co/datasets/enelpol/rag-mini-bioasq), which includes 4,719 question-answer passages from the BioASQ challenges on biomedical semantic indexing and question answering (QA) [dataset for task b BioASQ11](http://participants-area.bioasq.org/datasets/), which can be used as *Positive Pair* configuration.\n",
    "\n",
    "We have to load the dataset, and we can do it using the HF datasets library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from HF hub\n",
    "\n",
    "# (anchor, positive, negative)\n",
    "all_nli_triplet_train = load_dataset(\"sentence-transformers/all-nli\", \"triplet\", split=\"train[:500]\")\n",
    "# (sentence1, sentence2) + score\n",
    "stsb_pair_score_train = load_dataset(\"sentence-transformers/stsb\", split=\"train[:500]\")\n",
    "\n",
    "# (anchor, positive, negative)\n",
    "all_nli_triplet_dev = load_dataset(\"sentence-transformers/all-nli\", \"triplet\", split=\"dev[:400]\")\n",
    "# (sentence1, sentence2, score)\n",
    "stsb_pair_score_dev = load_dataset(\"sentence-transformers/stsb\", split=\"validation[:400]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载自定义FAQ数据集，并通过 in-batch 和 hard-negtive-sampling 策略生成负样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mine hard negatives\n",
    "# https://github.com/UKPLab/sentence-transformers/releases/tag/v3.1.0\n",
    "# pip install sentence-transformers[train]==3.1.1\n",
    "# RuntimeError: The NVIDIA driver on your system is too old\n",
    "from sentence_transformers.util import mine_hard_negatives\n",
    "def hard_negtive_sampling(dataset):\n",
    "    dataset = mine_hard_negatives(\n",
    "        dataset=dataset,\n",
    "        model=model,\n",
    "        range_min=10,\n",
    "        range_max=50,\n",
    "        max_score=0.8,  # 负样本的最高相似得分，可用于控制难易程度\n",
    "        relative_margin=0.05,         # 0.05 means that the negative is at most 95% as similar to the anchor as the positive\n",
    "        num_negatives=5,  # 10 or less is recommended\n",
    "        sampling_strategy=\"random\",      # \"top\" means that we sample the top candidates as negatives\n",
    "        batch_size=128,        # Adjust as needed\n",
    "        use_faiss=True,               # Optional: Use faiss/faiss-gpu for faster\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "# 加载自定义数据集\n",
    "custom_samples = pd.read_csv(\n",
    "    \"....csv\", \n",
    "    header=0, sep=\",\", encoding=\"utf-8\", index_col=False\n",
    ")\n",
    "from datasets import Dataset\n",
    "custom_dataset = Dataset.from_dict({\n",
    "    \"anchor\": demo_samples['standard_question'],\n",
    "    \"positive\": demo_samples['similar_question'],\n",
    "})\n",
    "\n",
    "custom_dataset = hard_negtive_sampling(custom_dataset)\n",
    "# (anchor, positive, negative)\n",
    "\n",
    "# 切分训练集和测试集\n",
    "custom_dataset = custom_dataset.train_test_split(test_size=0.1, seed=123, shuffle=True)\n",
    "custom_dataset_train = custom_dataset[\"train\"]\n",
    "custom_dataset_dev = custom_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all datasets into a dictionary with dataset names to datasets\n",
    "train_dataset = {\n",
    "    \"all-nli-triplet\": all_nli_triplet_train,\n",
    "    \"stsb\": stsb_pair_score_train,\n",
    "    \"custom\": custom_dataset_train\n",
    "}\n",
    "\n",
    "eval_dataset = {\n",
    "    \"all-nli-triplet\": all_nli_triplet_dev,\n",
    "    \"stsb\": stsb_pair_score_dev, \n",
    "    \"custom\": custom_dataset_dev\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model evaluator that will be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "from sentence_transformers import evaluation\n",
    "\n",
    "model_evaluator = evaluation.TripletEvaluator(\n",
    "    anchors=eval_dataset['custom'][\"anchor\"],\n",
    "    positives=eval_dataset['custom'][\"positive\"],\n",
    "    negatives=eval_dataset['custom'][\"negative\"],\n",
    "    name=\"all_nli_dev\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss function that will be used for training\n",
    "\n",
    "In this case, we are using the MultipleNegativesRankingLoss to fine-tune our embedding model. This choice is based on our dataset format, which consists of positive text pairs. You can take a look at [dataset format](https://sbert.net/docs/sentence_transformer/training_overview.html#dataset-format) information and [loss function](https://sbert.net/docs/sentence_transformer/loss_overview.html) information to determine which loss function to use based on your use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.losses import CoSENTLoss, MultipleNegativesRankingLoss\n",
    "\n",
    "# (anchor, positive), (anchor, positive, negative)\n",
    "mnrl_loss = MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# (sentence_A, sentence_B) + score\n",
    "cosent_loss = CoSENTLoss(model)\n",
    "\n",
    "# Create a mapping with dataset names to loss functions, so the trainer knows which loss to apply where.\n",
    "losses={\n",
    "    \"all-nli-triplet\": mnrl_loss,\n",
    "    \"stsb\": cosent_loss,\n",
    "    \"custom\": mnrl_loss,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune embedding model with SentenceTransformersTrainer\n",
    "\n",
    "Now that we've prepared our data and model, we're ready to fine-tune our embedding model using the SentenceTransformersTrainer.\n",
    "\n",
    "To configure our training process, we'll use the SentenceTransformerTrainingArguments class. This tool allows us to specify various parameters that can impact training performance and help with tracking and debugging. We'll be using parameter values based on those recommended in the [Sentence Transformers documentation](https://sbert.net/docs/sentence_transformer/training_overview.html#training-arguments). However, it's important to note that these are just starting points. For optimal results, you should experiment with different values tailored to your specific dataset and task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "\n",
    " \n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=os.getenv(\"WANDB_NAME\"), # Save checkpoints\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,   # Loading model in mixed-precision\n",
    "    bf16=False,  # Set to True if you have a GPU that supports BF16\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # losses that use \"in-batch negatives\" benefit from no duplicates\n",
    "    # Optional tracking/debugging parameters:\n",
    "    eval_strategy=\"steps\",  # transformers 版本 4.41.0 中引入的\n",
    "    eval_steps=500,         # 每隔多少步的训练进行一次验证(执行evaluator)\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,   # save checkpoints during training\n",
    "    save_total_limit=3,\n",
    "    logging_steps=500,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=os.getenv('WANDB_NAME'),\n",
    "\n",
    "    load_best_model_at_end=True, # 如果设置为 True，在训练结束时加载根据评估器确定的最佳模型. defaults to `False`\n",
    "    metric_for_best_model='eval_loss',   # 和 `load_best_model_at_end`联合使用，模型对比. eval_pearson_cosine\n",
    "    greater_is_better=False,    # 跟前面两个参数一起使用\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformerTrainer\n",
    " \n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    loss=losses,\n",
    "    # evaluator=model_evaluator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training the model\n",
    "trainer.train()\n",
    "\n",
    "# save the model\n",
    "model.save_pretrained(\"./sbert-model/final\")\n",
    " \n",
    "# #  The model will be saved to the hub and the output directory\n",
    "# trainer.save_model()\n",
    "\n",
    "# # (Optional) Push it to the Hugging Face Hub\n",
    "# trainer.model.push_to_hub(os.getenv(\"WANDB_NAME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training on 4k samples took around 1 minute on an Nvidia A10G instance of [Modal labs](https://modal.com/pricing). At the time of writing (July 2024), the instance costs 1.1 USD/hour which indicates a cost of less than 0.1 USD for the training.\n",
    "\n",
    "What's pending now is the evaluation of the fine-tuned model using the 'model evaluator' from earlier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    " \n",
    "fine_tuned_model = SentenceTransformer(\n",
    "    args.output_dir, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "from sentence_transformers import evaluation\n",
    "evaluator = evaluation.TripletEvaluator(\n",
    "    anchors=eval_dataset['custom'][\"anchor\"],\n",
    "    positives=eval_dataset['custom'][\"positive\"],\n",
    "    negatives=eval_dataset['custom'][\"negative\"],\n",
    "    name=\"all_nli_dev\",\n",
    ")\n",
    " \n",
    "fine_tuned_results = evaluator(fine_tuned_model)\n",
    "fine_tuned_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we focus on only a couple of metrics that are more relevant in our case, we get the following information:\n",
    "\n",
    "| Model | MRR@10 | NDCG@10 |\n",
    "|-------|--------|---------|\n",
    "| all-mpnet-base-v2 (Baseline) | 0.8347 | 0.8571 |\n",
    "| bge-base-en-v1.5 | 0.8965 | 0.9122 |\n",
    "| all-mpnet-base-v2 Fine-tuned | 0.8919 | 0.9093 |\n",
    "\n",
    "The fine-tuned model shows significant improvements over the baseline model, with a 6.85% increase in MRR@10 and a 6.09% increase in NDCG@10. It reached the performance level of the bge-base-en-v1.5 embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Embedding models play a crucial role in the success of Retrieval-Augmented Generation (RAG) applications, as the quality of retrieved context directly impacts the generated answers. Using the Sentence Transformers 3 library, we fine-tuned the all-mpnet-base-v2 model on a biomedical question-answering dataset. The results show substantial improvements:\n",
    "\n",
    "- MRR@10 increased from 0.8347 to 0.8919 (6.85% improvement)\n",
    "- NDCG@10 improved from 0.8571 to 0.9093 (6.09% improvement)\n",
    "\n",
    "Our fine-tuned model achieved performance comparable to the more advanced bge-base-en-v1.5 model despite starting from a lower baseline.\n",
    "\n",
    "The fine-tuning process has become highly accessible and efficient. With only 4,719 question-answer pairs, we were able to achieve these improvements in approximately 1 minute of training time on an Nvidia A10G GPU. The estimated cost for this training was less than 0.1 USD, making it a cost-effective approach for enhancing domain-specific retrieval tasks.\n",
    "This shows the value of customizing embedding models for specific domains or use cases. Significant performance gains can be realized even with a relatively small dataset and minimal training time. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
